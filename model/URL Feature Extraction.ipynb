{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ipaddress\n",
        "import re\n",
        "import socket\n",
        "import ssl\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import whois\n",
        "from datetime import datetime\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def haveAtSign(url):\n",
        "    return 1 if \"@\" in url else 0\n",
        "\n",
        "def getLength(url):\n",
        "    return 1 if len(url) >= 54 else 0\n",
        "\n",
        "def getDepth(url):\n",
        "    try:\n",
        "        s = urlparse(url).path.split('/')\n",
        "        depth = sum(1 for part in s if len(part) != 0)\n",
        "        return depth\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def redirection(url):\n",
        "    try:\n",
        "        pos = url.rfind('//')\n",
        "        return 1 if pos > 6 else 0\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def httpDomain(url):\n",
        "    try:\n",
        "        domain = urlparse(url).netloc\n",
        "        return 1 if 'https' in domain else 0\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def tinyURL(url):\n",
        "    try:\n",
        "        shortening_services = r\"bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|\" \\\n",
        "                            r\"yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|\" \\\n",
        "                            r\"short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|\" \\\n",
        "                            r\"doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|db\\.tt|\" \\\n",
        "                            r\"qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|q\\.gs|is\\.gd|\" \\\n",
        "                            r\"po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|x\\.co|\" \\\n",
        "                            r\"prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|\" \\\n",
        "                            r\"tr\\.im|link\\.zip\\.net\"\n",
        "        match = re.search(shortening_services, url)\n",
        "        return 1 if match else 0\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def prefixSuffix(url):\n",
        "    try:\n",
        "        return 1 if '-' in urlparse(url).netloc else 0\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def havingIP(url):\n",
        "    try:\n",
        "        ipaddress.ip_address(urlparse(url).netloc)\n",
        "        return 1\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def get_whois_info(domain):\n",
        "    try:\n",
        "        whois_info = whois.whois(domain)\n",
        "        return whois_info\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def dnsRecord(domain_name):\n",
        "    return 0 if domain_name else 1\n",
        "\n",
        "def domainAge(domain_name):\n",
        "    try:\n",
        "        if not domain_name or not domain_name.creation_date:\n",
        "            return 1\n",
        "        creation_date = domain_name.creation_date\n",
        "        if isinstance(creation_date, list):\n",
        "            creation_date = creation_date[0]\n",
        "        age_in_days = (datetime.now() - creation_date).days\n",
        "        return 1 if (age_in_days / 30) < 6 else 0\n",
        "    except:\n",
        "        return 1\n",
        "\n",
        "def domainEnd(domain_name):\n",
        "    try:\n",
        "        if not domain_name or not domain_name.expiration_date:\n",
        "            return 1\n",
        "        expiration_date = domain_name.expiration_date\n",
        "        if isinstance(expiration_date, list):\n",
        "            expiration_date = expiration_date[0]\n",
        "        end_in_days = (expiration_date - datetime.now()).days\n",
        "        return 0 if (end_in_days / 30) < 6 else 1\n",
        "    except:\n",
        "        return 1\n",
        "\n",
        "def get_url_response(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10, allow_redirects=True)\n",
        "        return response\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def iframe(response_text):\n",
        "    return 0 if response_text and re.findall(r\"<iframe|<frameBorder>\", response_text) else 1\n",
        "\n",
        "def mouseOver(response_text):\n",
        "    return 1 if response_text and re.findall(r\"onmouseover\", response_text) else 0\n",
        "\n",
        "def rightClick(response_text):\n",
        "    return 0 if response_text and re.findall(r\"event.button ?== ?2\", response_text) else 1\n",
        "\n",
        "def forwarding(response):\n",
        "    return 1 if response and len(response.history) > 2 else 0\n",
        "\n",
        "def has_login_form(response_text):\n",
        "    try:\n",
        "        soup = BeautifulSoup(response_text, 'html.parser')\n",
        "        form_tags = soup.find_all('form')\n",
        "        for form in form_tags:\n",
        "            if any(field.get('type') in ['password', 'email'] for field in form.find_all('input')):\n",
        "                return 1\n",
        "        return 0\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def get_form_action_url(response_text, base_url):\n",
        "    try:\n",
        "        soup = BeautifulSoup(response_text, 'html.parser')\n",
        "        form_tags = soup.find_all('form')\n",
        "        for form in form_tags:\n",
        "            action = form.get('action')\n",
        "            if action:\n",
        "                action_url = urljoin(base_url, action)\n",
        "                if urlparse(action_url).netloc != urlparse(base_url).netloc:\n",
        "                    return 1\n",
        "        return 0\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def get_suspicious_keywords(response_text):\n",
        "    try:\n",
        "        suspicious_words = r'login|account|verify|update|urgent|password|security|billing'\n",
        "        return 1 if re.search(suspicious_words, response_text, re.IGNORECASE) else 0\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def webTraffic(url):\n",
        "    # A API da Alexa não é mais funcional. Usamos um valor de placeholder.\n",
        "    return 1 if url else 0\n",
        "\n",
        "def extract_all_features(url, label):\n",
        "    features = {\n",
        "        \"Domain\": urlparse(url).netloc if url else None,\n",
        "        \"Have_IP\": havingIP(url),\n",
        "        \"Have_At\": haveAtSign(url),\n",
        "        \"URL_Length\": getLength(url),\n",
        "        \"URL_Depth\": getDepth(url),\n",
        "        \"Redirection\": redirection(url),\n",
        "        \"https_Domain\": 1 if url.startswith('https') else 0,\n",
        "        \"TinyURL\": tinyURL(url),\n",
        "        \"Prefix/Suffix\": prefixSuffix(url),\n",
        "        \"DNS_Record\": 1,\n",
        "        \"Web_Traffic\": 0,\n",
        "        \"Domain_Age\": 1,\n",
        "        \"Domain_End\": 1,\n",
        "        \"iFrame\": 1,\n",
        "        \"Mouse_Over\": 0,\n",
        "        \"Right_Click\": 1,\n",
        "        \"Web_Forwards\": 0,\n",
        "        \"Has_LoginForm\": 0,\n",
        "        \"Form_Action_Suspect\": 0,\n",
        "        \"Suspicious_Keywords\": 0,\n",
        "        \"Label\": label\n",
        "    }\n",
        "    \n",
        "    domain = features['Domain']\n",
        "    if domain:\n",
        "        whois_info = get_whois_info(domain)\n",
        "        features['DNS_Record'] = dnsRecord(whois_info)\n",
        "        features['Domain_Age'] = domainAge(whois_info)\n",
        "        features['Domain_End'] = domainEnd(whois_info)\n",
        "    \n",
        "    response = get_url_response(url)\n",
        "    if response:\n",
        "        features['Web_Traffic'] = webTraffic(url)\n",
        "        features['iFrame'] = iframe(response.text)\n",
        "        features['Mouse_Over'] = mouseOver(response.text)\n",
        "        features['Right_Click'] = rightClick(response.text)\n",
        "        features['Web_Forwards'] = forwarding(response)\n",
        "        features['Has_LoginForm'] = has_login_form(response.text)\n",
        "        features['Form_Action_Suspect'] = get_form_action_url(response.text, url)\n",
        "        features['Suspicious_Keywords'] = get_suspicious_keywords(response.text)\n",
        "    \n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting feature extraction...\n"
          ]
        }
      ],
      "source": [
        "print(\"Starting feature extraction...\")\n",
        "\n",
        "try:\n",
        "    phishurl_all = pd.read_csv(\"Datasets/2.online-valid.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'online-valid.csv' not found. Downloading...\")\n",
        "    os.system(\"wget http://data.phishtank.com/data/online-valid.csv\")\n",
        "    phishurl_all = pd.read_csv(\"Datasets/2.online-valid.csv\")\n",
        "\n",
        "phishurl = phishurl_all.sample(n=5000, random_state=12).copy().reset_index(drop=True)\n",
        "\n",
        "try:\n",
        "    legiurl_all = pd.read_csv(\"Datasets/1.Benign_list_big_final.csv\", header=None)\n",
        "    legiurl_all.columns = ['URLs']\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'Benign_list_big_final.csv' not found.\")\n",
        "    exit()\n",
        "\n",
        "legiurl = legiurl_all.sample(n=5000, random_state=12).copy().reset_index(drop=True)\n",
        "\n",
        "legi_features = [extract_all_features(url, 0) for url in legiurl['URLs']]\n",
        "legitimate = pd.DataFrame(legi_features)\n",
        "\n",
        "phish_features = [extract_all_features(url, 1) for url in phishurl['url']]\n",
        "phishing = pd.DataFrame(phish_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Dataset Head:\n",
            "              Domain  Have_IP  Have_At  URL_Length  URL_Depth  Redirection  \\\n",
            "0  graphicriver.net        0        0           1          1            0   \n",
            "1    foursquare.com        0        0           1          3            0   \n",
            "2       shop-pro.jp        0        0           1          6            0   \n",
            "3     motthegioi.vn        0        0           1          2            0   \n",
            "4        tobogo.net        0        0           1          2            0   \n",
            "\n",
            "   https_Domain  TinyURL  Prefix/Suffix  DNS_Record  ...  Domain_Age  \\\n",
            "0             0        0              0           1  ...           1   \n",
            "1             1        0              0           1  ...           1   \n",
            "2             0        0              1           1  ...           1   \n",
            "3             0        0              0           1  ...           1   \n",
            "4             0        0              0           1  ...           1   \n",
            "\n",
            "   Domain_End  iFrame  Mouse_Over  Right_Click  Web_Forwards  Has_LoginForm  \\\n",
            "0           1       1           0            1             0              0   \n",
            "1           1       1           0            1             0              0   \n",
            "2           1       1           0            1             0              0   \n",
            "3           1       1           0            1             0              0   \n",
            "4           1       1           0            1             0              0   \n",
            "\n",
            "   Form_Action_Suspect  Suspicious_Keywords  Label  \n",
            "0                    0                    0      0  \n",
            "1                    0                    1      0  \n",
            "2                    0                    0      0  \n",
            "3                    0                    0      0  \n",
            "4                    0                    0      0  \n",
            "\n",
            "[5 rows x 21 columns]\n",
            "\n",
            "Final Dataset Shape: (10000, 21)\n",
            "\n",
            "Dataset '6.full_urldata_features.csv' salvo com sucesso!\n",
            "\n",
            "--- Fim do Notebook ---\n"
          ]
        }
      ],
      "source": [
        "urldata = pd.concat([legitimate, phishing]).reset_index(drop=True)\n",
        "print(\"\\nFinal Dataset Head:\\n\", urldata.head())\n",
        "print(\"\\nFinal Dataset Shape:\", urldata.shape)\n",
        "\n",
        "urldata.to_csv('Datasets/6.full_urldata_features.csv', index=False)\n",
        "print(\"\\nDataset '6.full_urldata_features.csv' salvo com sucesso!\")\n",
        "\n",
        "print(\"\\n--- Fim do Notebook ---\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "URL Feature Extraction.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
